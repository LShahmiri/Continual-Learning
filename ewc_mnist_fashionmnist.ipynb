{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMQ4K/oZtkufAV9JsxrTS4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LShahmiri/Continual-Learning/blob/main/ewc_mnist_fashionmnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUR4z_vkS0Q7",
        "outputId": "6788489b-9e24-4a09-ac37-c3ab25731f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 20.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 497kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.61MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 13.4MB/s]\n",
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 200kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.75MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 27.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import autograd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_accuracy(model, dataloader):\n",
        "    model = model.eval()\n",
        "    acc = 0\n",
        "    for input, target in dataloader:\n",
        "        o = model(input.to(device))\n",
        "        acc += (o.argmax(dim=1).long() == target.to(device)).float().mean()\n",
        "    return acc / len(dataloader)\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "    # from https://github.com/shivamsaboo17/Overcoming-Catastrophic-forgetting-in-Neural-Networks/blob/master/elastic_weight_consolidation.py\n",
        "    def __init__(self, input_dim, output_dim, act='relu', use_bn=False):\n",
        "        super(LinearLayer, self).__init__()\n",
        "        self.use_bn = use_bn\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "        self.act = nn.ReLU() if act == 'relu' else act\n",
        "        if use_bn:\n",
        "            self.bn = nn.BatchNorm1d(output_dim)\n",
        "    def forward(self, x):\n",
        "        if self.use_bn:\n",
        "            return self.bn(self.act(self.lin(x)))\n",
        "        return self.act(self.lin(x))\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
        "        super(Model, self).__init__()\n",
        "        self.f1 = Flatten()\n",
        "        self.lin1 = LinearLayer(num_inputs, num_hidden, use_bn=True)\n",
        "        self.lin2 = LinearLayer(num_hidden, num_hidden, use_bn=True)\n",
        "        self.lin3 = nn.Linear(num_hidden, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin3(self.lin2(self.lin1(self.f1(x))))\n",
        "\n",
        "# Load MNIST dataset, representint task A\n",
        "mnist_train = datasets.MNIST(\"../data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(\"../data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)\n",
        "\n",
        "# FashiomMNIST is task B\n",
        "f_mnist_train = datasets.FashionMNIST(\"../data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "f_mnist_test = datasets.FashionMNIST(\"../data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "f_train_loader = DataLoader(f_mnist_train, batch_size = 100, shuffle=True)\n",
        "f_test_loader = DataLoader(f_mnist_test, batch_size = 100, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "EPOCHS = 4\n",
        "lr=0.001\n",
        "weight=100000\n",
        "accuracies = {}\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train model on task A\n",
        "model = Model(28 * 28, 100, 10).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "for _ in range(EPOCHS):\n",
        "    for input, target in tqdm(train_loader):\n",
        "        output = model(input.to(device))\n",
        "        loss = criterion(output, target.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "accuracies['mnist_initial'] = get_accuracy(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMRKyV1ETCpr",
        "outputId": "53a2d427-bf2a-4311-a2bd-5496346c9861"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:07<00:00, 76.71it/s]\n",
            "100%|██████████| 600/600 [00:07<00:00, 79.74it/s]\n",
            "100%|██████████| 600/600 [00:06<00:00, 88.11it/s]\n",
            "100%|██████████| 600/600 [00:07<00:00, 80.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-343cN1UcRS",
        "outputId": "839a6fe5-6613-46f4-b863-e22b1bb78031"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mnist_initial': tensor(0.9765, device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ewc_loss(model, weight, estimated_fishers, estimated_means):\n",
        "    losses = []\n",
        "    for param_name, param in model.named_parameters():\n",
        "        estimated_mean = estimated_means[param_name]\n",
        "        estimated_fisher = estimated_fishers[param_name]\n",
        "        losses.append((estimated_fisher * (param - estimated_mean) ** 2).sum())\n",
        "\n",
        "    return (weight / 2) * sum(losses)\n",
        "\n",
        "def estimate_ewc_params(model, train_ds, batch_size=100, num_batch=300, estimate_type='true'):\n",
        "    estimated_mean = {}\n",
        "\n",
        "    for param_name, param in model.named_parameters():\n",
        "        estimated_mean[param_name] = param.data.clone()\n",
        "\n",
        "    estimated_fisher = {}\n",
        "    dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "\n",
        "    for n, p in model.named_parameters():\n",
        "        estimated_fisher[n] = torch.zeros_like(p)\n",
        "\n",
        "    model.eval()\n",
        "    for i, (input, target) in enumerate(dl):\n",
        "        if i > num_batch:\n",
        "            break\n",
        "        model.zero_grad()\n",
        "\n",
        "        output = model(input.to(device))\n",
        "        # https://www.inference.vc/on-empirical-fisher-information/ - more on this here\n",
        "        if ESTIMATE_TYPE == 'empirical':\n",
        "            # empirical\n",
        "            label = target.to(device)\n",
        "        else:\n",
        "            # true estimate\n",
        "            label = output.max(1)[1]\n",
        "\n",
        "        loss = F.nll_loss(F.log_softmax(output, dim=1), label)\n",
        "        loss.backward()\n",
        "\n",
        "        # accumulate all the gradients\n",
        "        for n, p in model.named_parameters():\n",
        "            estimated_fisher[n].data += p.grad.data ** 2 / len(dl)\n",
        "\n",
        "    estimated_fisher = {n: p for n, p in estimated_fisher.items()}\n",
        "    return estimated_mean, estimated_fisher"
      ],
      "metadata": {
        "id": "x8TpuLIqTf7z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute fisher and mean parameters for EWC loss\n",
        "ESTIMATE_TYPE = 'true'\n",
        "estimated_mean, estimated_fisher = estimate_ewc_params(model, mnist_train)\n",
        "\n",
        "# Train task B fashion mnist\n",
        "for _ in range(EPOCHS):\n",
        "    for input, target in tqdm(f_train_loader):\n",
        "        output = model(input.to(device))\n",
        "        loss = ewc_loss(model, weight, estimated_fisher, estimated_mean) + criterion(output, target.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "accuracies['mnist_EWC'] = get_accuracy(model, test_loader)\n",
        "accuracies['f_mnist_EWC'] = get_accuracy(model, f_test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWhBnn57TjfC",
        "outputId": "55e6cd85-fe08-4827-d1d0-a91a109e9cc2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 600/600 [00:07<00:00, 75.76it/s]\n",
            "100%|██████████| 600/600 [00:08<00:00, 69.60it/s]\n",
            "100%|██████████| 600/600 [00:08<00:00, 69.88it/s]\n",
            "100%|██████████| 600/600 [00:08<00:00, 72.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CucRgCDUDoT",
        "outputId": "a229f948-d1c2-49b1-bb11-001a7bbe3057"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mnist_initial': tensor(0.9765, device='cuda:0'),\n",
              " 'mnist_EWC': tensor(0.9690, device='cuda:0'),\n",
              " 'f_mnist_EWC': tensor(0.8366, device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}